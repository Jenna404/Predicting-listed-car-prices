{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a106c2-f185-492f-87bb-79286c2eacc4",
   "metadata": {},
   "source": [
    "### __BUSA8001 (S2, 2023) Group Assignment - Predicting Used Car Sale Prices__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c316658-edf7-4797-b078-202987b4922b",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "**Kaggle Competition Ends:** Friday, 3 November 2023 @ 3:00pm (Week 13)   \n",
    "\n",
    "**Overview:**   \n",
    "\n",
    "- In the group assignment you will form a team of 3 members and participate in a forecasting competition on Kaggle\n",
    "- The goal is to predict prices of used cars based on car characteristics and regression models\n",
    "- Assessment Summary:  \n",
    "    - Write a problem statement and perform Exploratory Data Analysis  \n",
    "    - Clean up data, deal with categorical features and missing observations, and create new explanatory variables (feature engineering)  \n",
    "    - Construct and tune forecasting models, produce forecasts and submit your predictions to Kaggle  \n",
    "    - Each member of the team will record a video presentation of their work  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24de689-928d-4991-b337-760c12780e5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Problem Description and Initial Data Analysis\n",
    "\n",
    "1. Read the Competition Overview on Kaggle [https://www.kaggle.com/t/32b34f072642495487836cf93453ac6a](https://www.kaggle.com/t/32b34f072642495487836cf93453ac6a)\n",
    "2. Referring to Competition Overview and the data provided on Kaggle write **Problem Description** (about 500 words) focusing on key points that will need to be addressed as first steps in Tasks 2 and 3 below, \n",
    "\n",
    "- Using the following headings:\n",
    "    - Forecasting Problem - explain what you are trying to do and how it could be used in the real world (i.e. why it may be important)\n",
    "    - Evaluation Criteria - explain the criteria is used to assess forecast performance \n",
    "    - Types of Variables/Features\n",
    "    - Data summary and main data characteristics\n",
    "    - Missing Values (only explain what you find - do not impute missing values at this stage)\n",
    "    - You should **not** discuss any specific predictive algorithms at this stage\n",
    "    - Note: Your written portion of this task should be completed in a single Markdown cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ef70",
   "metadata": {},
   "source": [
    "##### 1. Forecasting Problem\n",
    "The objective of the forecasting problem, based on the dataset provided, is to predict the listing price of a vehicle based on various features, including its specifications, condition, location, and dealer information. This is a regression problem, as the outcome we're trying to predict (the vehicle's price) is continuous.\n",
    "\n",
    "In the real world, such a prediction model holds immense value. Dealerships and individual sellers can use it to determine the ideal listing price for a vehicle, ensuring it's neither underpriced (resulting in potential revenue loss) nor overpriced (leading to reduced interest from buyers). Buyers can also use the model to check if a listed vehicle is reasonably priced. Additionally, online car marketplaces can integrate this model to provide instant price suggestions to sellers or to highlight good deals to potential buyers.\n",
    "\n",
    "##### 2. Evaluation Criteria\n",
    "Although the Kaggle competition link is not directly accessible here, commonly used evaluation metrics for regression problems include the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared. RMSE gives an indication of the model's performance in terms of the magnitude of error. A lower RMSE indicates a better fit of the model to the data. For the sake of this exercise, let's assume RMSE is the chosen metric, as it penalizes large errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    "##### 3. Types of Variables/Features\n",
    "The dataset comprises a mix of categorical, numerical, and boolean features. Here's a breakdown:\n",
    "\n",
    "Categorical Features: body_type, city, engine_type, exterior_color, interior_color, listing_color, make_name, model_name, transmission, transmission_display, wheel_system.\n",
    "Numerical Features: city_fuel_economy, daysonmarket, dealer_zip, engine_displacement, highway_fuel_economy, horsepower, latitude, longitude, mileage, savings_amount, seller_rating, year.\n",
    "Boolean Features: franchise_dealer, has_accidents, is_new.\n",
    "String Features with Quantitative Information: Many features like back_legroom, front_legroom, fueltankvolume, height, length, power, torque, wheelbase, width are provided as strings but contain numeric data (with units).\n",
    "##### 4. Data Summary and Main Data Characteristics\n",
    "The dataset contains information about various vehicles, including their technical specifications, physical attributes, condition, and listing details. The target variable is the price of the vehicle. This dataset provides a comprehensive view of factors that potential buyers might consider, from the car's mileage and accident history to the dealer's rating.\n",
    "\n",
    "##### 5. Missing Values\n",
    "Before discussing missing values, let's first identify which columns have them and how many they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960bb55-92da-4528-9ee1-e06e46f00cae",
   "metadata": {},
   "source": [
    "`(Task 1, Text Here - insert more cells as required)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3999e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2236966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3500 entries, 0 to 3499\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   vin                   3500 non-null   object \n",
      " 1   back_legroom          3397 non-null   object \n",
      " 2   body_type             3494 non-null   object \n",
      " 3   city                  3500 non-null   object \n",
      " 4   city_fuel_economy     2912 non-null   float64\n",
      " 5   daysonmarket          3500 non-null   int64  \n",
      " 6   dealer_zip            3500 non-null   int64  \n",
      " 7   engine_displacement   3375 non-null   float64\n",
      " 8   engine_type           3450 non-null   object \n",
      " 9   exterior_color        3500 non-null   object \n",
      " 10  franchise_dealer      3500 non-null   bool   \n",
      " 11  front_legroom         3397 non-null   object \n",
      " 12  fuel_tank_volume      3397 non-null   object \n",
      " 13  fuel_type             3463 non-null   object \n",
      " 14  height                3397 non-null   object \n",
      " 15  highway_fuel_economy  2912 non-null   float64\n",
      " 16  horsepower            3375 non-null   float64\n",
      " 17  interior_color        3500 non-null   object \n",
      " 18  is_new                3500 non-null   bool   \n",
      " 19  latitude              3500 non-null   float64\n",
      " 20  length                3397 non-null   object \n",
      " 21  listed_date           3500 non-null   object \n",
      " 22  listing_color         3500 non-null   object \n",
      " 23  longitude             3500 non-null   float64\n",
      " 24  make_name             3500 non-null   object \n",
      " 25  maximum_seating       3397 non-null   object \n",
      " 26  mileage               3297 non-null   float64\n",
      " 27  model_name            3500 non-null   object \n",
      " 28  power                 3201 non-null   object \n",
      " 29  savings_amount        3500 non-null   int64  \n",
      " 30  seller_rating         3500 non-null   float64\n",
      " 31  torque                3169 non-null   object \n",
      " 32  transmission          3440 non-null   object \n",
      " 33  transmission_display  3440 non-null   object \n",
      " 34  wheel_system          3399 non-null   object \n",
      " 35  wheelbase             3397 non-null   object \n",
      " 36  width                 3397 non-null   object \n",
      " 37  year                  3500 non-null   int64  \n",
      " 38  price                 3500 non-null   int64  \n",
      "dtypes: bool(2), float64(8), int64(5), object(24)\n",
      "memory usage: 1018.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5ac9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vin                       0\n",
       "back_legroom            103\n",
       "body_type                 6\n",
       "city                      0\n",
       "city_fuel_economy       588\n",
       "daysonmarket              0\n",
       "dealer_zip                0\n",
       "engine_displacement     125\n",
       "engine_type              50\n",
       "exterior_color            0\n",
       "franchise_dealer          0\n",
       "front_legroom           103\n",
       "fuel_tank_volume        103\n",
       "fuel_type                37\n",
       "height                  103\n",
       "highway_fuel_economy    588\n",
       "horsepower              125\n",
       "interior_color            0\n",
       "is_new                    0\n",
       "latitude                  0\n",
       "length                  103\n",
       "listed_date               0\n",
       "listing_color             0\n",
       "longitude                 0\n",
       "make_name                 0\n",
       "maximum_seating         103\n",
       "mileage                 203\n",
       "model_name                0\n",
       "power                   299\n",
       "savings_amount            0\n",
       "seller_rating             0\n",
       "torque                  331\n",
       "transmission             60\n",
       "transmission_display     60\n",
       "wheel_system            101\n",
       "wheelbase               103\n",
       "width                   103\n",
       "year                      0\n",
       "price                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779234ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city_fuel_economy       588\n",
       "highway_fuel_economy    588\n",
       "torque                  331\n",
       "power                   299\n",
       "mileage                 203\n",
       "horsepower              125\n",
       "engine_displacement     125\n",
       "length                  103\n",
       "wheelbase               103\n",
       "maximum_seating         103\n",
       "back_legroom            103\n",
       "height                  103\n",
       "fuel_tank_volume        103\n",
       "front_legroom           103\n",
       "width                   103\n",
       "wheel_system            101\n",
       "transmission             60\n",
       "transmission_display     60\n",
       "engine_type              50\n",
       "fuel_type                37\n",
       "body_type                 6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the training data again\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ad345",
   "metadata": {},
   "source": [
    "Numeric Features: (e.g., `city_fuel_economy`, `highway_fuel_economy`, `torque`, `power`, `mileage`, `horsepower`, `engine_displacement`, `length`, `wheelbase`, etc.): For numeric features, we consider imputing missing values using statistical measures like the mean, median, or mode, depending on the distribution of the data. Imputing with the mean is a common approach and can be done for columns where the missing values are not too extensive. Categorical Features (e.g., `engine_type`, `fuel_type`, `body_type`, etc.): For categorical features, we impute missing values with the mode (most frequent category) or introduce a new category like \"Other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30df24-b18b-4f61-975f-ceebe2e2c3ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Data Cleaning, Missing Observations and Feature Engineering\n",
    "- In this task you will follow a set of instructions/questions listed below.\n",
    "- Make sure you explain each answer carefully both in Markdown text and on your video.\n",
    "\n",
    "Total Marks: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4ec3d-4936-4428-a204-04f133210a3d",
   "metadata": {},
   "source": [
    "**Task 2, Question 1**: Clean **all** numerical features so that they can be used in training algorithms. For instance, back_legroom feature is in object format containing both numerical values and text. Extract numerical values (equivalently eliminate the text) so that the numerical values can be used as a regular feature.  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d051dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3df20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(data):\n",
    "    # Features with units that need cleaning\n",
    "    features_with_units = ['back_legroom', 'front_legroom', 'fuel_tank_volume', 'height', 'length', 'wheelbase', 'width', 'maximum_seating']\n",
    "\n",
    "    # Extract numerical values from these features\n",
    "    for feature in features_with_units:\n",
    "        if data[feature].dtype == 'object':\n",
    "            data[feature] = data[feature].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "            \n",
    "    # Display the cleaned features\n",
    "    print(data[features_with_units].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313fef0-4043-4c03-8f3f-4691b5f61f11",
   "metadata": {},
   "source": [
    "In order to clean all the numerical features so that they can be used in training algorithms, we use the clean_feature function. The clean_feature function is designed to clean and preprocess a dataset, specifically focusing on certain features that have units. We first identifies a list of features that need cleaning. These features are 'back_legroom', 'front_legroom', 'fuel_tank_volume', 'height', 'length', 'wheelbase', 'width', and 'maximum_seating'. We check for if the data type of the feature is an object, if the feature is indeed an object, it applies a regular expression to extract the numerical part of the string. The regular expression (\\d+.?\\d*) is used to match any number that may or may not have a decimal point, then convert to the float data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f1bb1",
   "metadata": {},
   "source": [
    "**Task 2, Question 2** Create at least 5 new features from the existing numerical variables which contain multiple items of information, for example you could extract maximum torque and torque rpm from the torque variable.  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d210de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(data):\n",
    "    data['max_torque'] = pd.to_numeric(data['torque'].str.split().str[0])\n",
    "    data['torque_rpm'] = data['torque'].str.split().str[3]\n",
    "    data['torque_rpm'] = data['torque_rpm'].str.replace(',', '')\n",
    "    data['torque_rpm'] = pd.to_numeric(data['torque_rpm'])\n",
    "    \n",
    "    data['power_value'] = pd.to_numeric(data['power'].str.split().str[0])\n",
    "    data['power_rpm'] = data['power'].str.split().str[3]\n",
    "    data['power_rpm'] = data['power_rpm'].str.replace(',', '')\n",
    "    data['power_rpm'] = pd.to_numeric(data['power_rpm'])\n",
    "    \n",
    "    data['torque_to_power_ratio'] = data['max_torque'] / data['power_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fde9b6-bed0-4018-9edb-3553f3d1fa13",
   "metadata": {},
   "source": [
    "Creates a new feature max_torque by splitting the torque feature on spaces and taking the first item (str[0]) (which is assumed to be the maximum torque value), and converting it to a numeric type. We create another new feature torque_rpm by splitting the torque feature on spaces and taking the fourth item (which is assumed to be the RPM at which maximum torque is produced). It removes any commas from this value and converts it to a numeric type, for example 12,000 in to 12000. Similarly,a power_value feature is created by splitting the power feature on spaces, taking the first item (which is assumed to be the power value), and converting it to a numeric type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d01c4d-1412-4e51-8f80-7040f536b1c7",
   "metadata": {},
   "source": [
    "**Task 2, Question 3**: Impute missing values for all features in both the training and test datasets.   \n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36412d68-f68f-4764-aa6f-88ba33116ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to impute missing values\n",
    "def impute_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        # If column data type is object (categorical) then fill missing with mode\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "        # If column data type is numerical or boolean then fill missing with median\n",
    "        else:\n",
    "            df[column].fillna(df[column].median(), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb16b1d-f4f1-438e-a5af-ae9cc3c04467",
   "metadata": {},
   "source": [
    "Creates a new feature max_torque by splitting the torque feature on spaces and taking the first item (str[0]) (which is assumed to be the maximum torque value), and converting it to a numeric type. We create another new feature torque_rpm by splitting the torque feature on spaces and taking the fourth item (which is assumed to be the RPM at which maximum torque is produced). It removes any commas from this value and converts it to a numeric type, for example 12,000 in to 12000. Similarly,a power_value feature is created by splitting the power feature on spaces, taking the first item (which is assumed to be the power value), and converting it to a numeric type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c14ab6-aff9-4fcd-ab7b-34f0b77f8caa",
   "metadata": {},
   "source": [
    "**Task 2, Question 4**: Encode all categorical variables appropriately as discussed in class. \n",
    "\n",
    "- Where multiple values are given for an observation encode the observation as 'other'. \n",
    "- Where a categorical feature contains more than 5 unique values, map the features into 5 most frequent values + 'other' and then encode appropriately. For instance, map colours into 5 basic colours + 'other': [red, yellow, green, blue, purple, other] and then encode.  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91f4448-8574-4397-a636-d83bbde885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to encode categorical variables\n",
    "def encode_categorical_columns(df):\n",
    "    for column in df.select_dtypes(include=['object', 'bool']).columns:\n",
    "        # Set observations with multiple values to 'other'\n",
    "        df[column] = df[column].apply(lambda x: 'other' if '/' in str(x) else x)\n",
    "        \n",
    "        if df[column].dtype == bool:\n",
    "            df[column] = df[column].astype(int)\n",
    "            \n",
    "        # If there are more than 5 unique values, keep the top 5 and set the rest to 'other'\n",
    "        if df[column].nunique() > 5 and column != 'vin':\n",
    "            top_5_values = df[column].value_counts().head(5).index\n",
    "            #print(df[column].value_counts().head(5))\n",
    "            #print(top_5_values)\n",
    "            df[column] = df[column].apply(lambda x: x if x in top_5_values else 'other')\n",
    "    \n",
    "    # One-hot encode the categorical columns\n",
    "    dummies = pd.get_dummies(df.drop('vin', axis = 1), drop_first=True)\n",
    "\n",
    "    # Add the excluded column back to the dummies DataFrame\n",
    "    dummies['vin'] = df['vin']\n",
    "\n",
    "    return dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad3f20-7549-4b9e-8894-818c976d1457",
   "metadata": {},
   "source": [
    "A copy of the original DataFrame `car` is created and stored in `car_graph` for reference. Several columns, including 'power', 'torque', 'vin', 'latitude', 'longitude', and 'listed_date', are dropped from the `car` DataFrame. These columns are likely removed because they may not be relevant to the analysis or modeling task. The 'dealer_zip' column is converted to a string data type, possibly to treat it as a categorical variable during encoding. A function called `encode_categorical_columns` is defined to encode categorical variables. The function performs the following operations for each categorical column:\n",
    "   - Replaces values containing '/' with 'other'.\n",
    "   - Converts boolean values to integers (1 for True, 0 for False).\n",
    "   - Keeps the top 5 most frequent values in the column and replaces the rest with 'other'.\n",
    "   - Performs one-hot encoding, dropping the first category to avoid multicollinearity. \n",
    "   \n",
    "The `encode_categorical_columns` function is applied to the `car` DataFrame, and the result is stored in the `car_encoded` DataFrame. This DataFrame contains one-hot encoded features for the categorical variables. The code displays the first few rows of the `car_encoded` DataFrame to provide a preview of the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695347",
   "metadata": {},
   "source": [
    "**Task 2, Question 5**: Perform any other actions you think need to be done on the data before constructing predictive models, and clearly explain what you have done.   \n",
    "(1 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a02b43",
   "metadata": {},
   "source": [
    "##### 1. Feature Scaling:\n",
    "Machine learning algorithms perform better when numerical input variables are scaled to a standard range. This is particularly true for algorithms that rely on the magnitude of variables, such as gradient descent-based algorithms or distance-based algorithms like KNN.\n",
    "\n",
    "StandardScaler is a common method to scale features, which will transform the data such that its distribution has a mean value of 0 and a standard deviation of 1.\n",
    "\n",
    "##### 2. Handling Outliers:\n",
    "Outliers can adversely affect the performance of some models. We can use methods like the IQR (Inter-Quartile Range) to identify and remove outliers from the dataset.\n",
    "\n",
    "##### 3. Feature Engineering:\n",
    "Given the nature of the dataset, there might be some potential interactions between features that could be useful. For instance, creating a feature that combines city and highway fuel economy might be indicative.\n",
    "\n",
    "##### 4. Reducing Dimensionality:\n",
    "After one-hot encoding, the number of features in the dataset may increase significantly. Some of these features might be highly correlated, and using them all might lead to overfitting. Methods like PCA (Principal Component Analysis) can be employed to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   back_legroom  front_legroom  fuel_tank_volume  height  length  wheelbase  \\\n",
      "0          33.5           41.3              14.8    60.2   174.2      106.3   \n",
      "1          36.8           42.8              15.5    65.6   179.2      105.9   \n",
      "2          36.8           42.8              15.5    65.6   179.2      105.9   \n",
      "3          30.3           42.6              13.2    55.0   175.5      104.3   \n",
      "4          38.6           43.2              16.2    64.1   180.6      106.7   \n",
      "\n",
      "   width  maximum_seating  \n",
      "0   82.0              5.0  \n",
      "1   84.1              5.0  \n",
      "2   84.1              5.0  \n",
      "3   68.9              5.0  \n",
      "4   83.0              5.0  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "merged_df = pd.merge(train_df.drop(columns=['price']), test_df, how='outer')\n",
    "\n",
    "clean_features(merged_df)\n",
    "new_features(merged_df)\n",
    "impute_missing_values(merged_df)\n",
    "\n",
    "merged_ori = merged_df.copy()\n",
    "merged_df = merged_df.drop(['power', 'torque', 'listed_date'], axis = 1)\n",
    "merged_df['dealer_zip'] = merged_df['dealer_zip'].apply(str)\n",
    "merged_encoded = encode_categorical_columns(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a238a3-5432-4058-8b6e-1172c4e396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "columns_to_scale = merged_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Fit the scaler to the selected columns and transform the data\n",
    "\n",
    "merged_encoded_sc = merged_encoded.copy()\n",
    "merged_encoded_sc[columns_to_scale] = sc.fit_transform(merged_encoded_sc[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0572d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler \n",
    "scaler_encoded = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling directly to the identified numerical columns in the train and test datasets\n",
    "for col in columns_to_scale:\n",
    "    # Fit the scaler to the training data column and transform\n",
    "    merged_encoded[col] = scaler_encoded.fit_transform(merged_encoded[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a2da9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_legroom</th>\n",
       "      <th>city_fuel_economy</th>\n",
       "      <th>daysonmarket</th>\n",
       "      <th>engine_displacement</th>\n",
       "      <th>franchise_dealer</th>\n",
       "      <th>front_legroom</th>\n",
       "      <th>fuel_tank_volume</th>\n",
       "      <th>height</th>\n",
       "      <th>highway_fuel_economy</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>...</th>\n",
       "      <th>transmission_display_8-Speed Automatic</th>\n",
       "      <th>transmission_display_Automatic</th>\n",
       "      <th>transmission_display_Continuously Variable Transmission</th>\n",
       "      <th>transmission_display_other</th>\n",
       "      <th>wheel_system_4X2</th>\n",
       "      <th>wheel_system_AWD</th>\n",
       "      <th>wheel_system_FWD</th>\n",
       "      <th>wheel_system_RWD</th>\n",
       "      <th>vin</th>\n",
       "      <th>horsepower_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176282</td>\n",
       "      <td>0.148718</td>\n",
       "      <td>0.231392</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.180807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SJKCH5CRXHA032566</td>\n",
       "      <td>0.180807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774737</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.273992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5LMCJ3D96HUL54638</td>\n",
       "      <td>0.273992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774737</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.273992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5LMCJ2D95HUL35217</td>\n",
       "      <td>0.273992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.637895</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.147249</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.086231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2HGFG1B86AH500600</td>\n",
       "      <td>0.086231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812632</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.237179</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.294498</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.239221</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5LMCJ1D95LUL25032</td>\n",
       "      <td>0.239221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.709474</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.182051</td>\n",
       "      <td>0.132686</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.308762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WBAKF9C53CE672450</td>\n",
       "      <td>0.308762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.751579</td>\n",
       "      <td>0.136752</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.325243</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.083449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>KL7CJLSB1LB063169</td>\n",
       "      <td>0.083449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.755789</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0.168285</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.264256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>JTHBK1EG7C2488661</td>\n",
       "      <td>0.264256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.732632</td>\n",
       "      <td>0.145299</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.330128</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.177994</td>\n",
       "      <td>0.220183</td>\n",
       "      <td>0.157163</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5NPEF4JA3LH058405</td>\n",
       "      <td>0.157163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.774737</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176282</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.282337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1GNKRFED6GJ185504</td>\n",
       "      <td>0.282337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      back_legroom  city_fuel_economy  daysonmarket  engine_displacement  \\\n",
       "0         0.705263           0.094017      0.015974             0.166667   \n",
       "1         0.774737           0.076923      0.051118             0.216667   \n",
       "2         0.774737           0.076923      0.011182             0.216667   \n",
       "3         0.637895           0.128205      0.010383             0.133333   \n",
       "4         0.812632           0.094017      0.011182             0.166667   \n",
       "...            ...                ...           ...                  ...   \n",
       "4995      0.709474           0.076923      0.015974             0.333333   \n",
       "4996      0.751579           0.136752      0.190096             0.066667   \n",
       "4997      0.755789           0.076923      0.006390             0.416667   \n",
       "4998      0.732632           0.145299      0.022364             0.250000   \n",
       "4999      0.774737           0.042735      0.014377             0.433333   \n",
       "\n",
       "      franchise_dealer  front_legroom  fuel_tank_volume    height  \\\n",
       "0                  0.0       0.176282          0.148718  0.231392   \n",
       "1                  1.0       0.224359          0.166667  0.318770   \n",
       "2                  1.0       0.224359          0.166667  0.318770   \n",
       "3                  0.0       0.217949          0.107692  0.147249   \n",
       "4                  1.0       0.237179          0.184615  0.294498   \n",
       "...                ...            ...               ...       ...   \n",
       "4995               0.0       0.192308          0.182051  0.132686   \n",
       "4996               1.0       0.160256          0.128205  0.325243   \n",
       "4997               0.0       0.205128          0.243590  0.168285   \n",
       "4998               1.0       0.330128          0.176923  0.177994   \n",
       "4999               0.0       0.176282          0.333333  0.388350   \n",
       "\n",
       "      highway_fuel_economy  horsepower  ...  \\\n",
       "0                 0.137615    0.180807  ...   \n",
       "1                 0.110092    0.273992  ...   \n",
       "2                 0.110092    0.273992  ...   \n",
       "3                 0.211009    0.086231  ...   \n",
       "4                 0.146789    0.239221  ...   \n",
       "...                    ...         ...  ...   \n",
       "4995              0.128440    0.308762  ...   \n",
       "4996              0.165138    0.083449  ...   \n",
       "4997              0.137615    0.264256  ...   \n",
       "4998              0.220183    0.157163  ...   \n",
       "4999              0.082569    0.282337  ...   \n",
       "\n",
       "      transmission_display_8-Speed Automatic  transmission_display_Automatic  \\\n",
       "0                                        0.0                             0.0   \n",
       "1                                        0.0                             0.0   \n",
       "2                                        0.0                             0.0   \n",
       "3                                        0.0                             0.0   \n",
       "4                                        1.0                             0.0   \n",
       "...                                      ...                             ...   \n",
       "4995                                     0.0                             0.0   \n",
       "4996                                     0.0                             0.0   \n",
       "4997                                     0.0                             1.0   \n",
       "4998                                     1.0                             0.0   \n",
       "4999                                     0.0                             0.0   \n",
       "\n",
       "      transmission_display_Continuously Variable Transmission  \\\n",
       "0                                                   0.0         \n",
       "1                                                   0.0         \n",
       "2                                                   0.0         \n",
       "3                                                   0.0         \n",
       "4                                                   0.0         \n",
       "...                                                 ...         \n",
       "4995                                                0.0         \n",
       "4996                                                0.0         \n",
       "4997                                                0.0         \n",
       "4998                                                0.0         \n",
       "4999                                                0.0         \n",
       "\n",
       "      transmission_display_other  wheel_system_4X2  wheel_system_AWD  \\\n",
       "0                            1.0               0.0               1.0   \n",
       "1                            0.0               0.0               1.0   \n",
       "2                            0.0               0.0               1.0   \n",
       "3                            1.0               0.0               0.0   \n",
       "4                            0.0               0.0               1.0   \n",
       "...                          ...               ...               ...   \n",
       "4995                         1.0               0.0               1.0   \n",
       "4996                         0.0               0.0               0.0   \n",
       "4997                         0.0               0.0               0.0   \n",
       "4998                         0.0               0.0               0.0   \n",
       "4999                         0.0               0.0               0.0   \n",
       "\n",
       "      wheel_system_FWD  wheel_system_RWD                vin  \\\n",
       "0                  0.0               0.0  SJKCH5CRXHA032566   \n",
       "1                  0.0               0.0  5LMCJ3D96HUL54638   \n",
       "2                  0.0               0.0  5LMCJ2D95HUL35217   \n",
       "3                  1.0               0.0  2HGFG1B86AH500600   \n",
       "4                  0.0               0.0  5LMCJ1D95LUL25032   \n",
       "...                ...               ...                ...   \n",
       "4995               0.0               0.0  WBAKF9C53CE672450   \n",
       "4996               1.0               0.0  KL7CJLSB1LB063169   \n",
       "4997               1.0               0.0  JTHBK1EG7C2488661   \n",
       "4998               1.0               0.0  5NPEF4JA3LH058405   \n",
       "4999               1.0               0.0  1GNKRFED6GJ185504   \n",
       "\n",
       "      horsepower_squared  \n",
       "0               0.180807  \n",
       "1               0.273992  \n",
       "2               0.273992  \n",
       "3               0.086231  \n",
       "4               0.239221  \n",
       "...                  ...  \n",
       "4995            0.308762  \n",
       "4996            0.083449  \n",
       "4997            0.264256  \n",
       "4998            0.157163  \n",
       "4999            0.282337  \n",
       "\n",
       "[5000 rows x 88 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "quadratic_features = quadratic.fit_transform(merged_encoded[['horsepower']])\n",
    "\n",
    "# create a new column 'horsepower_squared' with the quadratic features\n",
    "merged_encoded['horsepower_squared'] = quadratic_features[:, 1]\n",
    "\n",
    "merged_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a081c-cdc4-4fc8-9bd7-52bfd4c3e6dd",
   "metadata": {},
   "source": [
    "We apply feature scaling to the numerical features in the training data (`train_df_encoded`) using the `fit_transform` method. `train_df_encoded` is assumed to be a DataFrame containing both numerical and categorical columns. `common_numerical_cols` is expected to be a list of column names that correspond to the numerical features. This means that the numerical columns in `train_df_encoded` will be standardized. \n",
    "\n",
    "   The `fit_transform` method does two things:\n",
    "   - It computes the mean and standard deviation of each numerical column in the `common_numerical_cols` from the training data.\n",
    "   - It then standardizes the data by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "Then apply the same scaler to transform the numerical features in the test data (`test_df_encoded`) using the `transform` method. This ensures that the scaling applied to the test data is consistent with the scaling used on the training data. This step is crucial to avoid data leakage and ensure that the model can be tested on unseen data.\n",
    "\n",
    "After running this code, numerical features in both the training and test datasets will be standardized, with a mean of 0 and a standard deviation of 1. This normalization can help improve the performance and stability of various machine learning algorithms when applied to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16475c",
   "metadata": {},
   "source": [
    "**Task 2, Question 6**: Perform exploratory data analysis to measure the relationship between the features and the target and carefully write up your findings. \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1509b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in /Applications/anaconda3/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /Applications/anaconda3/lib/python3.11/site-packages (from folium) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Applications/anaconda3/lib/python3.11/site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/pct404/.local/lib/python3.11/site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.11/site-packages (from folium) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from jinja2>=2.9->folium) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->folium) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->folium) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->folium) (2024.2.2)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install folium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a map centered on a specific latitude and longitude\u001b[39;00m\n\u001b[1;32m      6\u001b[0m m \u001b[38;5;241m=\u001b[39m folium\u001b[38;5;241m.\u001b[39mMap(location\u001b[38;5;241m=\u001b[39m[train_df_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], train_df_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]], zoom_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "! pip install folium\n",
    "\n",
    "import folium\n",
    "\n",
    "# Create a map centered on a specific latitude and longitude\n",
    "m = folium.Map(location=[train_df_graph['latitude'].iloc[0], train_df_graph['longitude'].iloc[0]], zoom_start=10)\n",
    "\n",
    "# Add markers for each point\n",
    "for index, row in train_df_graph.iterrows():\n",
    "    folium.Marker([row['latitude'], row['longitude']], popup=row['dealer_zip']).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file or display it\n",
    "m.save('map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['back_legroom', 'city_fuel_economy', 'daysonmarket', 'engine_displacement', 'front_legroom',\n",
    "                            'fuel_tank_volume', 'height', 'highway_fuel_economy', 'horsepower',\n",
    "                            'length', 'maximum_seating', 'mileage',\n",
    "                            'savings_amount', 'seller_rating', 'wheelbase', 'width',\n",
    "                            'max_torque', 'torque_rpm', 'power_value', 'power_rpm']\n",
    "\n",
    "# Extracting the correlation values of the corrected features with 'price'\n",
    "correlation_matrix = train_df_encoded[['price'] + features].corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Corrected Features with Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(train_df_encoded[['back_legroom', 'city_fuel_economy', 'daysonmarket', 'engine_displacement', 'front_legroom',\n",
    "                            'fuel_tank_volume', 'height', 'highway_fuel_economy', 'horsepower']])\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30391691",
   "metadata": {},
   "outputs": [],
   "source": [
    "'length', 'maximum_seating', 'mileage',\n",
    "                            'savings_amount', 'seller_rating', 'wheelbase', 'width',\n",
    "                            'max_torque', 'torque_rpm', 'power_value', 'power_rpm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a965eb-e661-4772-af72-98f19e1bae41",
   "metadata": {},
   "source": [
    "Visualize the Distribution of the Target Variable (price): A histogram or kernel density plot can help us understand the distribution of the target variable.\n",
    "\n",
    "Correlation Analysis: We'll compute the correlation of each feature with the target variable to identify which features are most strongly associated with the price.\n",
    "\n",
    "Visualize Relationships for Key Features: For the top features (based on correlation), we'll visualize their relationship with the target variable using scatter plots or boxplots.\n",
    "\n",
    "Visualize Categorical Features: For key categorical features, we can use boxplots to understand the distribution of the target variable across different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f6968-d9c2-422f-9935-eba856b3c028",
   "metadata": {},
   "source": [
    "--- \n",
    "## Task 3: Fit and tune a forecasting model/Submit predictions/Report score and ranking\n",
    "\n",
    "Make sure you **clearly explain each step** you do both in text and on the recoded video.   \n",
    "This task must not create any additional features and has to use on the dataset constructed in Task 2.\n",
    "\n",
    "1. Build at least 3 machine learning (ML) regression models taking into account the outcomes of Tasks 1 & 2 (Explain Carefully)\n",
    "2. Fit the models and tune hyperparameters via cross-validation: make sure you comment and explain each step clearly\n",
    "3. Select your best algorithm, create predictions using the test dataset, and submit your predictions on Kaggle's competition page\n",
    "4. Provide Kaggle ranking and **score** (screenshot your best submission) and comment\n",
    "5. Make sure your Python code works, so that a marker that can replicate your all Kaggle Score   \n",
    "\n",
    "- Hint: to perform well you will need to iterate Tasks 2 and Task 3.\n",
    "\n",
    "Total Marks: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53af94-d769-4a1d-a4a1-433f3dc7b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b89186",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_encoded[merged_encoded['vin'].isin(train_df['vin'])].drop('vin', axis=1).values\n",
    "\n",
    "y = train_df['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    #'Linear Regression': LinearRegression(),\n",
    "    #'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    #'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "    #'Ridge Regression': Ridge(random_state=42),\n",
    "    #'Gradient Boosted Regression Trees': GradientBoostingRegressor(random_state=42),\n",
    "    #'Kernel Ridge Regressor' : KernelRidge(random_state=42),\n",
    "    #'Support Vector Regressor' : SVR(random_state=42),\n",
    "    #'Bagging Regressor' : BaggingRegressor(random_state=42),\n",
    "    #'Adaptive Boost Regressor' : AdaBoostRegressor(random_state=42),\n",
    "    #'Multi-layer Perceptron Regressor' : MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the parameters for grid search\n",
    "params = {\n",
    "    #'Linear Regression': { },\n",
    "    #'Random Forest': { 'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [2, 5, 10] },\n",
    "    #'XGBoost': { 'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 7, 9], 'alpha': [0.1, 0.5, 1.0, 5.0, 10.0] },\n",
    "    #'Ridge Regression': { 'alpha': [0.1, 0.5, 1.0, 5.0, 10.0] },\n",
    "    #'Gradient Boosted Regression Trees': {\n",
    "    #'n_estimators': [50, 100, 250],\n",
    "    #'learning_rate': [0.01, 0.05, 0.15],\n",
    "    #'max_depth': [3, 4, 7, 9],\n",
    "    #'min_samples_split': [2, 5, 10],\n",
    "    #'min_samples_leaf': [2, 5, 10],\n",
    "    #'subsample': [0.8, 0.9, 1.0],\n",
    "    #'max_features': ['sqrt', 0.8],\n",
    "    #'Kernel Ridge Regressor': {'alpha': [0.1, 1.0, 10.0],'kernel': ['linear', 'polynomial', 'rbf'], 'gamma': [0.1, 1.0, 10.0]},\n",
    "    #'Support Vector Regressor': {'kernel': ['linear', 'rbf', 'poly'], 'C': [0.1, 1.0, 10.0], 'gamma': [0.1, 1.0, 10.0]},\n",
    "    #'Bagging Regressor': {'n_estimators': [10, 50, 100], 'max_samples': [0.5, 1.0]},\n",
    "    #'Adaptive Boost Regressor': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]},\n",
    "    #'Multi-layer Perceptron Regressor': {'hidden_layer_sizes': [(50,), (100, 50), (100, 100, 50)], 'activation': ['relu', 'tanh']},\n",
    "}\n",
    "\n",
    "# Initialize an empty dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# Perform grid search\n",
    "for model_name in models:\n",
    "    model = models[model_name]\n",
    "    param_grid = params[model_name]\n",
    "    random_search = RandomizedSearchCV(model, param_grid, n_iter=10, scoring='neg_mean_squared_error', cv=KFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[model_name] = random_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    print(f'Best model for {model_name}: {random_search.best_estimator_}')\n",
    "    print(f\"Best {model_name} cross-validation score (RMSE): {np.sqrt(np.abs(random_search.best_score_))}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacde3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cabfa",
   "metadata": {},
   "source": [
    "## Searching for the best hyperparatermers for each model\n",
    "\n",
    "### Linear regression\n",
    "\n",
    "- Best parameters for Linear Regression: {}\n",
    "- Best Linear Regression cross-validation score (MSE): 85402556.17530693\n",
    "- Linear Regression Test MSE: 125657738.31148525\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Two hyperparameter sets were identified from two gridsearches, one with cv = 5, the other with cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "- Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "- Best Random Forest cross-validation score (MSE): 36089024.152263604\n",
    "- Random Forest Test MSE: 67024115.424000144\n",
    "\n",
    "- Best parameters for Random Forest (KFold): {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "\n",
    "### XG Boost\n",
    "\n",
    "Two hyperparameter sets were identified from two gridsearches, one with cv = 5, the other with cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "- Best parameters for XGBoost: {'alpha': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
    "- Best XGBoost cross-validation score (MSE): 33958151.307104185\n",
    "- XGBoost Test MSE: 71178404.95253536\n",
    "\n",
    "- Best parameters for XGBoost (KFold): {'alpha': 1, 'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 200}\n",
    "\n",
    "\n",
    "### Gradient boosted regression trees\n",
    "- Best parameters for Gradient Boosted Regression Trees: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "- Best Gradient Boosted Regression Trees cross-validation score (MSE): 34216888.000975125\n",
    "\n",
    "- Best parameters for Gradient Boosted Regression Trees (KFold): {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "- Best Gradient Boosted Regression Trees cross-validation score (MSE): 28012239.107196353\n",
    "- Gradient Boosted Regression Trees Test MSE: 76421709.90472016\n",
    "\n",
    "\n",
    "### Bagging regressor\n",
    "\n",
    "- Best parameters for Bagging Regressor: {'max_samples': 1.0, 'n_estimators': 100}\n",
    "- Best Bagging Regressor cross-validation score (MSE): 42958938.22874457\n",
    "\n",
    "\n",
    "- Best parameters for Bagging Regressor: {'max_samples': 1.0, 'n_estimators': 50}\n",
    "- Best Bagging Regressor cross-validation score (MSE): 27744856.929287888\n",
    "- Bagging Regressor Test MSE: 77610694.74220997\n",
    "\n",
    "### Other models\n",
    "\n",
    "Because GridSearchCV takes a really long time to run, I wasn't able to run the whole cell in one go, instead I have to pick some models for each run.\n",
    "\n",
    "I also have to do GridSearchCV on a simpler training dataset (with only the top 20 variables included in training)\n",
    "\n",
    "From a first pass, I elimited the following models because they clearly performed worse than the other models:\n",
    "- 'Ridge Regression': Ridge(),\n",
    "- 'Kernel Ridge Regressor' : KernelRidge(),\n",
    "- 'Support Vector Regressor' : SVR(),\n",
    "- 'Adaptive Boost Regressor' : AdaBoostRegressor(),\n",
    "- 'Multi-layer Perceptron Regressor' : MLPRegressor()\n",
    "\n",
    "Best parameters for Ridge Regression: {'alpha': 5.0}\n",
    "Best model for Ridge Regression: Ridge(alpha=5.0)\n",
    "Best Ridge Regression cross-validation score (RMSE): 8712.027317339205\n",
    "\n",
    "\n",
    "Best parameters for Kernel Ridge Regressor: {'kernel': 'linear', 'gamma': 10.0, 'alpha': 0.1}\n",
    "Best model for Kernel Ridge Regressor: KernelRidge(alpha=0.1, gamma=10.0)\n",
    "Best Kernel Ridge Regressor cross-validation score (RMSE): 8874.359661958888\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9995434",
   "metadata": {},
   "source": [
    "## Fitting the best models parameters found from Gridsearch and Randomsearch\n",
    "\n",
    "This part I will fit the chosen models from GridSearchCV and RandomSearch to the whole training set, and calculate the RMSE of each model to see which one does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd408e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest 1': RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf = 2, random_state=42),\n",
    "    'Random Forest 2': RandomForestRegressor(n_estimators=50, max_depth=None, min_samples_split=5, min_samples_leaf = 2, random_state=42),\n",
    "    'XG Boost 1': xgb.XGBRegressor(alpha=1, learning_rate=0.05, max_depth=9, n_estimators=200, random_state=42),\n",
    "    'XG Boost 2': xgb.XGBRegressor(alpha=0.1, learning_rate=0.1, max_depth=5, n_estimators=200, random_state=42),\n",
    "    'Gradient Boosted 1' : GradientBoostingRegressor(learning_rate=0.1, max_depth=7, min_samples_leaf=2, min_samples_split=10,\n",
    "                              n_estimators=200, random_state=42),\n",
    "    'Gradient Boosted 2' : GradientBoostingRegressor(learning_rate=0.15, max_depth=4, max_features=0.8,\n",
    "                          min_samples_leaf=2, min_samples_split=6,\n",
    "                          n_estimators=250, random_state=42, subsample=0.8),\n",
    "    'Bagging Regressor' : BaggingRegressor(max_samples=1, n_estimators=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def fit_model(model, X, X_test, y, y_test):\n",
    "    model.fit(X, y)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    print(f'RMSE train: {r2_score(y_train, pred_train):.3f}, test: {r2_score(y_test, pred_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    model = models[model_name]\n",
    "    print(model_name)\n",
    "    fit_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c7e64",
   "metadata": {},
   "source": [
    "## A quick look\n",
    "\n",
    "First having a quick look at all the models:\n",
    "\n",
    "RMSE for each model\n",
    "\n",
    "Random Forest 1\n",
    "RMSE train: 0.978, test: 0.8030\n",
    "\n",
    "Random Forest 2\n",
    "RMSE train: 0.976, test: 0.8042\n",
    "\n",
    "XG Boost 1\n",
    "RMSE train: 0.996, test: 0.8012\n",
    "\n",
    "XG Boost 2\n",
    "RMSE train: 0.986, test: 0.8235\n",
    "\n",
    "Gradient Boosted 1\n",
    "RMSE train: 0.996, test: 0.8181\n",
    "\n",
    "Gradient Boosted 2\n",
    "RMSE train: 0.987, test: 0.8347\n",
    "\n",
    "Bagging Regressor\n",
    "RMSE train: -0.003, test: -0.0240\n",
    "\n",
    "We can see that basically the models perform with the full set of features, and there is a superior Random forest and XGBoost and Gradient Boosted hyperparameter set.\n",
    "\n",
    "Out of all models, Gradient boosted 2 is performing the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e29b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb3 = GradientBoostingRegressor(learning_rate=0.15, max_depth=4, max_features=0.8,\n",
    "                          min_samples_leaf=2, min_samples_split=6,\n",
    "                          n_estimators=250, random_state=42, subsample=0.8)\n",
    "\n",
    "fit_model(gb3, X, X_test, y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_merged = merged_encoded[merged_encoded['vin'].isin(test_df['vin'])].drop('vin', axis=1)\n",
    "\n",
    "y_pred_merged_gb3 = gb3.predict(X_test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['price'] = y_pred_merged_gb3\n",
    "\n",
    "test_df[['vin', 'price']].to_csv('submission_gb3_merged_minmax_quad.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68711ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(gb3, open(\"gb3_merged.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9f63b",
   "metadata": {},
   "source": [
    "## Stacking Regressor\n",
    "\n",
    "This part I try to use stacking regressor with the two best performing models. It looks like the model does well on the training set, but not too well on test set. It could be overfitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "gb3 = GradientBoostingRegressor(learning_rate=0.15, max_depth=4, max_features=0.8,\n",
    "                          min_samples_leaf=2, min_samples_split=6,\n",
    "                          n_estimators=250, random_state=42, subsample=0.8)\n",
    "\n",
    "xg2 = xgb.XGBRegressor(alpha=0.1, learning_rate=0.1, max_depth=5, n_estimators=200, random_state=42)\n",
    "\n",
    "estimators = [('Gradient Boosted',  gb3), ('XGBoost', xg2)]\n",
    "            \n",
    "stacking_reg = StackingRegressor(estimators=estimators, final_estimator = DecisionTreeRegressor(random_state =42))\n",
    "\n",
    "stacking_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_stacking = stacking_reg.predict(X_train)\n",
    "y_test_pred_stacking = stacking_reg.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'RMSE train: {r2_score(y_train, y_train_pred_stacking):.3f}, test: {r2_score(y_test, y_test_pred_stacking):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f5b7a",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "This part helps to illustrate the most influential features in the model using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf = 2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importance scores\n",
    "train = train_df_encoded.copy().drop(['price'], axis = 1)\n",
    "feature_names = train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance scores in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Create a bar plot to visualize feature importances\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance in Random Forest Regressor')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top\n",
    "plt.show()\n",
    "\n",
    "# Display the sorted feature importances\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349d3bf",
   "metadata": {},
   "source": [
    "## PCA, KPCA and LDA\n",
    "\n",
    "This part uses feature extraction to see if MSE is improved by reducing the dimensionality of the dataset.\n",
    "\n",
    "It looks like none of the methods helped to improve MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78267fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 13)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "gb3.fit(X_train_pca, y_train)\n",
    "\n",
    "gb3_pca_pred = gb3.predict(X_test_pca)\n",
    "\n",
    "gb3_pca_rmse = r2_score(y_test, gb3_pca_pred)\n",
    "\n",
    "print(gb3_pca_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=13, kernel='rbf', gamma=15)\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "X_test_kpca = kpca.transform(X_test)\n",
    "\n",
    "gb3.fit(X_train_kpca, y_train)\n",
    "\n",
    "gb3_kpca_pred = gb3.predict(X_test_kpca)\n",
    "\n",
    "gb3_kpca_rmse = r2_score(y_test, gb3_kpca_pred)\n",
    "\n",
    "print(gb3_kpca_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components = 13)\n",
    "\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "gb3.fit(X_train_lda, y_train)\n",
    "\n",
    "gb3_lda_pred = gb3.predict(X_test_lda)\n",
    "\n",
    "gb3_lda_rmse = r2_score(y_test, gb3_lda_pred)\n",
    "\n",
    "print(gb3_lda_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acced3",
   "metadata": {},
   "source": [
    "## Final conclusions\n",
    "\n",
    "We can see that XGBoost on merged dataset did the best on the test set.\n",
    "\n",
    "Eventhough Gradient Boosted performs slightly better on training set, it is not as good as XGBoost on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
